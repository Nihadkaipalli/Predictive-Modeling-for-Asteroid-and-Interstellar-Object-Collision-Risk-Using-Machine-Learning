{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPeqNUbX56MWQYYvlRpTK/o"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Clh3bFNPD6CY",
        "outputId": "00ca6f46-dbb4-4997-b7d6-2d3d07b46a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (677494, 79)\n",
            "First 5 Rows:\n",
            "    diameter  extent  albedo  rot_per  GM  BV  UB  IR spec_B  spec_T  ...  \\\n",
            "0       NaN     NaN     NaN      NaN NaN NaN NaN NaN    NaN     NaN  ...   \n",
            "1       NaN     NaN     NaN      NaN NaN NaN NaN NaN    NaN     NaN  ...   \n",
            "2       NaN     NaN     NaN      NaN NaN NaN NaN NaN    NaN     NaN  ...   \n",
            "3       NaN     NaN     NaN      NaN NaN NaN NaN NaN    NaN     NaN  ...   \n",
            "4       NaN     NaN     NaN      NaN NaN NaN NaN NaN    NaN     NaN  ...   \n",
            "\n",
            "        rms  two_body  A1  A1_sigma  A2  A2_sigma  A3  A3_sigma  DT  DT_sigma  \n",
            "0  0.000533       NaN NaN       NaN NaN       NaN NaN       NaN NaN       NaN  \n",
            "1  0.071263       NaN NaN       NaN NaN       NaN NaN       NaN NaN       NaN  \n",
            "2  0.000002       NaN NaN       NaN NaN       NaN NaN       NaN NaN       NaN  \n",
            "3       NaN         T NaN       NaN NaN       NaN NaN       NaN NaN       NaN  \n",
            "4  0.339380       NaN NaN       NaN NaN       NaN NaN       NaN NaN       NaN  \n",
            "\n",
            "[5 rows x 79 columns]\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (if using Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/My Drive/Asteroid Collision Dataset V2.csv'\n",
        "df = pd.read_csv(file_path, low_memory=False)\n",
        "\n",
        "# Inspect the dataset\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"First 5 Rows:\\n\", df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Cleaning and Preprocessing"
      ],
      "metadata": {
        "id": "FSkjg3op3ecd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Missing Data Analysis"
      ],
      "metadata": {
        "id": "3xKMqMbe3gl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze missing data\n",
        "missing_summary = df.isnull().mean().sort_values(ascending=False) * 100\n",
        "missing_summary = missing_summary[missing_summary > 0]\n",
        "print(\"\\nMissing Data Summary (Percentage of Missing Values):\")\n",
        "print(missing_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCGDZzB9F2E6",
        "outputId": "6afd7676-b863-4966-deac-949500a8d3b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing Data Summary (Percentage of Missing Values):\n",
            "DT_sigma          100.000000\n",
            "extent            100.000000\n",
            "GM                100.000000\n",
            "BV                100.000000\n",
            "spec_T            100.000000\n",
            "IR                100.000000\n",
            "K1                100.000000\n",
            "K2                100.000000\n",
            "PC                100.000000\n",
            "DT                100.000000\n",
            "UB                100.000000\n",
            "M1                100.000000\n",
            "G                 100.000000\n",
            "M2                100.000000\n",
            "name               99.999852\n",
            "prefix             99.999410\n",
            "A3                 99.998967\n",
            "A3_sigma           99.998967\n",
            "A1                 99.997491\n",
            "A1_sigma           99.997491\n",
            "spec_B             99.992620\n",
            "A2                 99.976531\n",
            "A2_sigma           99.976531\n",
            "n_del_obs_used     99.910700\n",
            "n_dop_obs_used     99.910700\n",
            "rot_per            99.801179\n",
            "two_body           99.062427\n",
            "albedo             99.040582\n",
            "diameter_sigma     98.987002\n",
            "diameter           98.986559\n",
            "H_sigma            15.064635\n",
            "sigma_per           0.937868\n",
            "sigma_ad            0.937868\n",
            "pha                 0.937720\n",
            "sigma_i             0.937573\n",
            "sigma_q             0.937573\n",
            "sigma_a             0.937573\n",
            "sigma_tp            0.937573\n",
            "sigma_e             0.937573\n",
            "moid                0.937573\n",
            "moid_ld             0.937573\n",
            "moid_jup            0.937573\n",
            "sigma_ma            0.937573\n",
            "sigma_w             0.937573\n",
            "sigma_om            0.937573\n",
            "sigma_n             0.937573\n",
            "data_arc            0.392771\n",
            "H                   0.146717\n",
            "condition_code      0.001033\n",
            "neo                 0.000295\n",
            "t_jup               0.000295\n",
            "per_y               0.000295\n",
            "per                 0.000295\n",
            "ad                  0.000295\n",
            "rms                 0.000148\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Drop Columns with Excessive Missing Values"
      ],
      "metadata": {
        "id": "taIG0ZAX3rFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with >90% missing values or irrelevant\n",
        "cols_to_drop_missing = missing_summary[missing_summary > 90].index.tolist()\n",
        "irrelevant_columns = ['producer', 'equinox', 'orbit_id', 'pdes', 'full_name', 'name', 'prefix']\n",
        "cols_to_drop = list(set(cols_to_drop_missing + irrelevant_columns))\n",
        "\n",
        "df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
        "print(\"\\nDropped Columns:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0oJSXZ6GIlx",
        "outputId": "1dd656b0-cb17-4331-b719-65d169c07cce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dropped Columns:\n",
            " - PC\n",
            " - A2\n",
            " - M2\n",
            " - name\n",
            " - G\n",
            " - n_dop_obs_used\n",
            " - orbit_id\n",
            " - producer\n",
            " - K1\n",
            " - K2\n",
            " - diameter\n",
            " - GM\n",
            " - two_body\n",
            " - A1_sigma\n",
            " - diameter_sigma\n",
            " - pdes\n",
            " - A2_sigma\n",
            " - A1\n",
            " - spec_T\n",
            " - full_name\n",
            " - equinox\n",
            " - A3_sigma\n",
            " - albedo\n",
            " - spec_B\n",
            " - A3\n",
            " - M1\n",
            " - n_del_obs_used\n",
            " - UB\n",
            " - prefix\n",
            " - rot_per\n",
            " - DT_sigma\n",
            " - extent\n",
            " - DT\n",
            " - IR\n",
            " - BV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGCBqqfdq5JO",
        "outputId": "f926f700-517a-44d9-ef2e-3789468c2888"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          a         e          i          om           w         q        ad  \\\n",
            "0  3.344072  0.333618  17.631497  191.717418  341.109523  2.228428  4.459715   \n",
            "1  2.149638  0.251325   4.728640  134.342070  281.563658  1.609380  2.689895   \n",
            "2  2.320503  0.138476   6.549856  273.834058  130.418938  1.999169  2.641838   \n",
            "3  2.904675  0.253684  13.139290  322.997330   64.837530  2.167804  3.641545   \n",
            "4  2.257216  0.145324   4.223982  205.857738  134.532417  1.929189  2.585243   \n",
            "\n",
            "      per_y  data_arc  condition_code  ...  sigma_w  sigma_ma  sigma_ad  \\\n",
            "0  6.115353      34.0             8.0  ...   1.1161   0.47857  0.032768   \n",
            "1  3.151781       4.0             9.0  ...  33.7820  11.31700  0.435020   \n",
            "2  3.534936      31.0             7.0  ...   2.0890  23.32100  0.004895   \n",
            "3  4.950561      27.0             NaN  ...      NaN       NaN       NaN   \n",
            "4  3.391313      10.0             9.0  ...  10.3150   8.20790  0.094289   \n",
            "\n",
            "    sigma_n sigma_tp  sigma_per  class   first_obs    last_obs       rms  \n",
            "0  0.001776   1.0621    24.6170    OMB  1927-06-01  1927-07-05  0.000533  \n",
            "1  0.075861  35.0930   279.2600    MCA  1935-10-19  1935-10-23  0.071263  \n",
            "2  0.000775  83.3740     3.5881    MBA  1937-02-10  1937-03-13  0.000002  \n",
            "3       NaN      NaN        NaN    MBA  1939-09-15  1939-10-12       NaN  \n",
            "4  0.015900  28.6600    67.7660    MBA  1942-09-05  1942-09-15  0.339380  \n",
            "\n",
            "[5 rows x 44 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Data Analysis\n",
        "missing_summary = df.isnull().mean().sort_values(ascending=False) * 100\n",
        "missing_summary = missing_summary[missing_summary > 0]\n",
        "print(\"\\nMissing Data Summary (Percentage of Missing Values):\")\n",
        "print(missing_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSfe3D0_rjfN",
        "outputId": "6b0c55f1-b8f9-4a52-fef1-9bf326fb5575"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing Data Summary (Percentage of Missing Values):\n",
            "H_sigma           15.064635\n",
            "sigma_per          0.937868\n",
            "sigma_ad           0.937868\n",
            "pha                0.937720\n",
            "moid_ld            0.937573\n",
            "moid_jup           0.937573\n",
            "sigma_tp           0.937573\n",
            "moid               0.937573\n",
            "sigma_w            0.937573\n",
            "sigma_om           0.937573\n",
            "sigma_q            0.937573\n",
            "sigma_i            0.937573\n",
            "sigma_a            0.937573\n",
            "sigma_e            0.937573\n",
            "sigma_ma           0.937573\n",
            "sigma_n            0.937573\n",
            "data_arc           0.392771\n",
            "H                  0.146717\n",
            "condition_code     0.001033\n",
            "per                0.000295\n",
            "per_y              0.000295\n",
            "neo                0.000295\n",
            "t_jup              0.000295\n",
            "ad                 0.000295\n",
            "rms                0.000148\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Convert Columns to Numeric"
      ],
      "metadata": {
        "id": "kvsCm3nj38HG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Convert columns to numeric where possible\n",
        "columns_to_clean = ['H', 'e', 'a', 'q', 'i', 'om', 'w', 'ma', 'n']\n",
        "for col in columns_to_clean:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')"
      ],
      "metadata": {
        "id": "Ud_PwlNArh8P"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. Impute Missing Values"
      ],
      "metadata": {
        "id": "NRcOqnMe4DdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values in numeric columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df[numeric_cols] = imputer.fit_transform(df[numeric_cols])"
      ],
      "metadata": {
        "id": "TKjjTidMrqMG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Feature Engineering"
      ],
      "metadata": {
        "id": "77bLo5fP5bgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Binary Mapping"
      ],
      "metadata": {
        "id": "O09U-J-H6TUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map binary columns\n",
        "binary_mapping = {'Y': 1, 'N': 0}\n",
        "for col in ['neo', 'pha']:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].map(binary_mapping)"
      ],
      "metadata": {
        "id": "nUlg2acO6ccu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Handle Dates"
      ],
      "metadata": {
        "id": "i3FcGjDa6dp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle date columns\n",
        "date_cols = ['epoch_cal', 'tp_cal', 'first_obs', 'last_obs']\n",
        "for col in date_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y-%m-%d')\n",
        "        df[f'{col}_year'] = df[col].dt.year\n",
        "        df[f'{col}_month'] = df[col].dt.month\n",
        "        df[f'{col}_day'] = df[col].dt.day\n",
        "        df.drop(columns=[col], inplace=True)"
      ],
      "metadata": {
        "id": "vTWooEcM6gJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remaining_categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "print(\"\\nRemaining Categorical Columns after Date conversion:\", remaining_categorical_cols)"
      ],
      "metadata": {
        "id": "Xbmj-I-g7b4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. One-Hot Encoding"
      ],
      "metadata": {
        "id": "QUqIpPxu6_lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Identify Remaining Categorical Columns\n",
        "remaining_categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "print(\"\\nRemaining Categorical Columns after Binary Mapping:\", remaining_categorical_cols)"
      ],
      "metadata": {
        "id": "1cgaIdfN7AvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the 'class' column\n",
        "if 'class' in df.columns:\n",
        "    df = pd.get_dummies(df, columns=['class'], prefix='class', drop_first=True)"
      ],
      "metadata": {
        "id": "i-izvNfF7OjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique values in the 'class' column\n",
        "if 'class' in df.columns:\n",
        "    print(\"Unique values in 'class' column:\", df['class'].unique())"
      ],
      "metadata": {
        "id": "UKatkcEb7ydt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. Derive New Features"
      ],
      "metadata": {
        "id": "OZSFDaAs58V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GRAVITATIONAL_CONSTANT = 39.478\n",
        "\n",
        "# Add derived features\n",
        "df['relative_orbital_distance'] = np.abs(df['q'] - df['a'] * (1 - df['e']))\n",
        "df['orbital_velocity'] = np.sqrt(GRAVITATIONAL_CONSTANT / df['a'])\n",
        "df['adjusted_eccentricity'] = df['e'] * df['a'] / df['q']\n",
        "\n",
        "# Perturbed features\n",
        "for col in ['a', 'e', 'q', 'i']:\n",
        "    sigma_col = f'sigma_{col}'\n",
        "    if sigma_col in df.columns:\n",
        "        df[f'{col}_perturbed'] = df[col] + np.random.normal(0, df[sigma_col])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH4xPYohWkKd",
        "outputId": "152ca54a-4286-49e8-97aa-29c3a649e67c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sqrt\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Define Features and Target"
      ],
      "metadata": {
        "id": "tIlU4ieP5dte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Create Collision Risk"
      ],
      "metadata": {
        "id": "4UXaa08S7_rM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create collision_risk based on MOID\n",
        "df['collision_risk'] = np.where(df['moid'] < 0.05, 1, 0)"
      ],
      "metadata": {
        "id": "8ILV0epw8C-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if collision_risk was created successfully\n",
        "print(df[['moid', 'collision_risk']])"
      ],
      "metadata": {
        "id": "0M73Q-0s5riu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Define Features (X) and Target (y)"
      ],
      "metadata": {
        "id": "HZIwuz3S8Kcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "features_to_drop = ['moid', 'moid_ld']  # Exclude MOID-related features\n",
        "X = df.drop(columns=features_to_drop + ['collision_risk'])\n",
        "y = df['collision_risk']\n",
        "\n",
        "# Ensure only numeric features\n",
        "X = X.select_dtypes(include=[np.number])"
      ],
      "metadata": {
        "id": "HDRQuDHL8Oou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Data Splitting and Scaling"
      ],
      "metadata": {
        "id": "8acFs0W_6RlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1. Split Dataset"
      ],
      "metadata": {
        "id": "bbkLe9ES6Tmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "n5l7Alif6Wym",
        "outputId": "d683d15d-36d4-4f44-a8fe-c7afb458391c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-1acb6e7441ef>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Split the dataset into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Reimpute Missing values"
      ],
      "metadata": {
        "id": "qxOukc1E3DM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Re-impute missing values in training and testing sets\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
        "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)"
      ],
      "metadata": {
        "id": "p9UEFyp43GJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3. Resample Using SMOTE"
      ],
      "metadata": {
        "id": "Tr2fh6Xp6ZYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yys3dKZAflaL",
        "outputId": "04352457-6de6-4456-8850-c9d2a46b687b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Resampled Training Set Shape:\", X_train_resampled.shape, y_train_resampled.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "GckPuA3a6cdV",
        "outputId": "7c208968-e355-479a-d326-b0b97c4e5057"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-cec2f957c260>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Resample the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msmote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Resampled Training Set Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_resampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_resampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \"\"\"\n\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0marrays_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArraysTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         self.sampling_strategy_ = check_sampling_strategy(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0maccept_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicate_one_vs_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         )\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1302\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1065\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             )\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify Resampling"
      ],
      "metadata": {
        "id": "TRlN6-tSmKmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify class distribution\n",
        "print(\"Class Distribution After Resampling:\\n\", pd.Series(y_train_resampled).value_counts())"
      ],
      "metadata": {
        "id": "h_EM9OSDl3n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check for Non-Numeric Data:"
      ],
      "metadata": {
        "id": "QccawqYVl9Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_numeric_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "print(\"Non-Numeric Columns in X_train:\", non_numeric_cols)"
      ],
      "metadata": {
        "id": "WBZ6oHEWl_Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check for Missing Values in the Dataset"
      ],
      "metadata": {
        "id": "H6guWn_fmBxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing Values in the Entire Dataset:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "id": "cqo7UfqkmFKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "ZysQpwLAcLyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qgq3nPvRcPHm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check class distribution\n",
        "class_distribution = y.value_counts(normalize=True) * 100\n",
        "print(\"Class Distribution (%):\\n\", class_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VwxFehoh9dV",
        "outputId": "cb621792-52fb-4065-8484-0e6333f137fb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution (%):\n",
            " collision_risk\n",
            "0    97.280271\n",
            "1     2.719729\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop moid and moid_ld From Features"
      ],
      "metadata": {
        "id": "Muj90DRQpvXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove MOID-related features\n",
        "X_no_moid = X.drop(columns=['moid', 'moid_ld'])\n",
        "\n",
        "# Verify the shape of the new feature matrix\n",
        "print(\"Feature Matrix Shape Without MOID:\", X_no_moid.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F4-n3g1pwqu",
        "outputId": "be917e3e-5bf7-4a41-ba8d-220d2f97c8c7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Matrix Shape Without MOID: (677494, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Derived Features Implementation\n",
        "\n",
        "# 1. Relative Orbital Distance\n",
        "X_no_moid['relative_orbital_distance'] = np.abs(X['q'] - X['a'] * (1 - X['e']))\n",
        "\n",
        "# 2. Orbital Velocity\n",
        "GRAVITATIONAL_CONSTANT = 39.478  # Gravitational constant in AU^3 / day^2\n",
        "X_no_moid['orbital_velocity'] = np.sqrt(GRAVITATIONAL_CONSTANT / X['a'])\n",
        "\n",
        "# 3. Adjusted Eccentricity\n",
        "X_no_moid['adjusted_eccentricity'] = X['e'] * X['a'] / X['q']\n",
        "\n",
        "# 4. Perturbed Parameters\n",
        "for col in ['a', 'e', 'q', 'i']:\n",
        "    sigma_col = f'sigma_{col}'\n",
        "    if sigma_col in X.columns:\n",
        "        X_no_moid[f'{col}_perturbed'] = X[col] + np.random.normal(0, X[sigma_col])\n",
        "\n",
        "print(\"Derived features added to the dataset.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXh07XVxzMyW",
        "outputId": "20e22c9e-aa49-445e-9059-90d1398e133d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Derived features added to the dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sqrt\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add derived features to the resampled training and testing sets\n",
        "derived_features = ['relative_orbital_distance', 'orbital_velocity', 'adjusted_eccentricity'] + \\\n",
        "                   [f'{col}_perturbed' for col in ['a', 'e', 'q', 'i'] if f'{col}_perturbed' in X_no_moid.columns]\n",
        "\n",
        "X_train_no_moid[derived_features] = X_no_moid.loc[X_train_no_moid.index, derived_features]\n",
        "X_test_no_moid[derived_features] = X_no_moid.loc[X_test_no_moid.index, derived_features]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "eXRmTJQizOY-",
        "outputId": "2169ef84-e5b2-4232-be81-bc3b0dfd449d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_no_moid' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-dd38c7788549>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0;34m[\u001b[0m\u001b[0;34mf'{col}_perturbed'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'e'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'q'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'i'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34mf'{col}_perturbed'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_no_moid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train_no_moid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mderived_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_no_moid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train_no_moid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderived_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_test_no_moid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mderived_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_no_moid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test_no_moid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderived_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_no_moid' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resample the Dataset Using SMOTE"
      ],
      "metadata": {
        "id": "pCikil6lp4z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "# Initialize SMOTE\n",
        "smote = SMOTE(random_state=42)"
      ],
      "metadata": {
        "id": "3DOXaprVr1Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resample with SMOTE after adding derived features\n",
        "X_resampled_no_moid, y_resampled_no_moid = smote.fit_resample(X_train_no_moid, y_train)\n",
        "\n",
        "print(\"Resampled dataset includes derived features.\")"
      ],
      "metadata": {
        "id": "9iASJcGZjsgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Random Forest Model"
      ],
      "metadata": {
        "id": "F94EmSDwj31_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1. Train Random Forest"
      ],
      "metadata": {
        "id": "blM869VyeG_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train Random Forest\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train_resampled, y_train_resampled)"
      ],
      "metadata": {
        "id": "ZwYv7JseeIoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the Model"
      ],
      "metadata": {
        "id": "E5Cy8oRPqOHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "EugXZUtgefez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions on the Original Test Set"
      ],
      "metadata": {
        "id": "xmzCj3zmkL51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and evaluate\n",
        "y_pred = rf_model.predict(X_test)"
      ],
      "metadata": {
        "id": "x8e2qCMfqY1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Report"
      ],
      "metadata": {
        "id": "1usHYPdzqZ1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "MIGVit89qceH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "c2DL5ndaqfLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "1flajcboqjIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Importance Analysis"
      ],
      "metadata": {
        "id": "2D_oienjqofY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance analysis\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Feature importance analysis\n",
        "feature_importance_derived = pd.DataFrame({\n",
        "    'Feature': X_train_no_moid.columns,\n",
        "    'Importance': rf_model_derived.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display top 10 features\n",
        "print(\"Top 10 Most Important Features With Derived Features:\")\n",
        "print(feature_importance_derived.head(10))\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_derived['Feature'][:10], feature_importance_derived['Importance'][:10])\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"Top 10 Feature Importances With Derived Features\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XCU59SIoqutO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}